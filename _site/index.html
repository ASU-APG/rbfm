<!DOCTYPE html>
<html lang=" en">
<style>
    .custom-col {
        width: 20%;
    }

    .container {
        display: flex;
        justify-content: space-around;
        flex-wrap: wrap;
    }

    .card {
        display: flex;
        align-items: center;
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 16px;
        width: 350px;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        margin: 10px;
    }

    .card img {
        border-radius: 12px;
        width: 128px;
        height: 128px;
        object-fit: cover;
        margin-right: 16px;
    }

    .card .info {
        display: flex;
        flex-direction: column;
    }

    .card .info .name {
        font-size: 18px;
        font-weight: bold;
        margin: 0;
    }

    .card .info .position {
        font-size: 14px;
        color: #666;
        margin: 4px 0;
    }

    .card .info .icon {
        display: flex;
        align-items: center;
        color: #1976d2;
    }

    .card .info .icon svg {
        width: 24px;
        height: 24px;
        margin-right: 8px;
    }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Workshop on RBFM | 2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models.</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Workshop on RBFM" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models." />
<meta property="og:description" content="2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Workshop on RBFM" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Workshop on RBFM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models.","headline":"Workshop on RBFM","name":"Workshop on RBFM","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Workshop on RBFM" /></head>
<body><header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Workshop on RBFM</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path
                            d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
                    </svg>
                </span>
            </label>

            <div class="trigger"><a class="page-link" href="/call_for_paper/">Call For Papers</a><a class="page-link" href="/schedule/">Schedule</a></div>
        </nav></div>
</header><main class="page-content" aria-label="Content">
        <div class="wrapper">
            <div>
    <center>
        <h1>
            2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models
        </h1>
        </br>
        <h3>
            NeurIPS 2025 (Tentative)
        </h3>
        <h3>
            San Diego, USA
        </h3>
        <h3>
            Prior edition: <a href="https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/RBFM">NeurIPS 2024</a>
        </h3>
    </center>
</div>

<div>
    <h1 class="page-heading">Introduction</h1>
    <p>In 2025, multimodal AI has evolved dramatically, with foundation models now seamlessly integrating text, images,
        video, and audio to create more human-like understanding and generation capabilities. As these technologies
        continue to transform industries from healthcare to creative arts, the need for responsible development has
        never been more critical. Recent advancements in multimodal adaptation and generalization have opened new
        possibilities while simultaneously presenting complex challenges that require proactive solutions.</p>

    <p>The landscape of multimodal AI has fundamentally shifted since our last workshop. Major players like OpenAI,
        Google, and Meta have heavily invested in multimodal capabilities, with breakthrough models like GPT-4o, SORA,
        Imagen, and Veo2 generating
        realistic images and videos from text, Meta's SeamlessM4T translating speech and text in real time, and many
        more are
        processing and integrating multiple data types simultaneously. These advancements have accelerated adoption
        across industries, with Gartner estimating that 40% of generative AI offerings will be multimodal by 2027, up
        from just 1% in 2023.</p>


    <p>Our workshop aims to provide a platform for the community to establish responsible design principles for the next
        generation of multimodal foundation models. This year's goals include::
    </p>

    <ul>
        <li><b>Explore novel design principles</b> emphasizing responsibility and sustainability in multimodal
            generative models, aiming to reduce their extensive data and computational demands.</li>
        <li>Establishing best practices for <b>controllability evaluation</b> that go beyond general-purpose scores to
            assess
            fundamental skills across modalities.</li>
        <li>Exploring <b>multimodal test-time adaptation</b> and domain generalization techniques that enhance model
            reliability across diverse real-world scenarios.</li>
        <li>Developing frameworks for <b>responsible agentic multimodal systems</b> that can safely operate with greater
            autonomy while maintaining human oversight in critical decisions.</li>
        <li>Enhance the <b>robustness against adversarial</b> and backdoor attacks, thereby
            securing their integrity in adversarial environments.</li>
        <li>Identify the sources of reliability concerns, whether they stem from data quality, model
            architecture, or pre-training strategies.</li>
    </ul>
</div>


</br>
</br>

<div>
    <h1 class="page-heading">More details are coming soon! Stay tuned!</h1>
</div>

</br>
</br>

<!-- <div>
    <h1 class="page-heading">Rising Stars Lightning Talks</h1>
</div> -->
<!-- <div>
    <h1 class="page-heading">Panel</h1>
</div> -->

<!-- <div>
    <br>
    <h1 class="page-heading" style="text-align: center;">Schedule</h1>
    <hr>
    <br>

    <div class="footer-col-wrapper" style="">
        <div class="footer-col footer-col-1">
            <img src="https://yilundu.github.io/images/yilun3.png" width="128" height="128">
            <br>
            <p>
                <br>
                <b>Date:</b> 26th January
                <br>
                <b>Time:</b> 13:00 - 14:00 MST
                <br>
                <b>Location:</b> BYENG-361, ASU
                <br>
                <b>YouTube Recooding: </b> <a href="https://youtu.be/0ACcDrWJs8I?si=_d2LWCluFUJwLCWl">Link</a>
            </p>
            </p>
            <br>
            <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/MIT_logo.svg/2560px-MIT_logo.svg.png" width="128">
        </div>

        <div class="footer-col footer-col-3" style="width:60%">
            <b style="font-size: 21px;"><a href="https://yilundu.github.io/">Yilun Du</a></b></br>
            <b>PhD Candidate, MIT</b></br>

            <p><b>Title: </b>Generalizing Outside the Training Distribution through Compositional Generation</p>
            <p><b>Abstract: </b>Generative AI has led to stunning successes in recent years but is fundamentally limited by the amount of data available.  This is especially limiting in the embodied setting – where an agent must make decisions in completely new environments. In this talk, I’ll introduce the idea of compositional generative modeling, which can significantly reduce needed data requirements by building complex generative models from smaller constituents. I’ll first introduce the idea of energy-based models and illustrate how they enable compositional generative modeling. I’ll then illustrate how such compositional models enable the synthesis of complex plans in novel environments as well as complex visual scenes in unseen environments. Finally, I'll show how such compositionality can be applied to multimodal models to construct decision making systems that can hierarchically plan to solve long-horizon problems.</p>
        </div>



    </div>
    <hr>
    <br><div class="footer-col-wrapper" style="">
        <div class="footer-col footer-col-1">
            <img src="https://huanwang.tech/images/profile_v2.jpg" width="128" height="128">
            <br>
            <p>
                <br>
                <b>Date:</b> 9th February
                <br>
                <b>Time:</b> 14:00 - 15:00 MST
                <br>
                <b>Location:</b> BYENG-361, ASU
                <br>
                <b>YouTube Recooding: </b> <a href="">Link</a>
            </p>
            </p>
            <br>
            <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Northeastern_seal.svg/800px-Northeastern_seal.svg.png" width="128">
        </div>

        <div class="footer-col footer-col-3" style="width:60%">
            <b style="font-size: 21px;"><a href="https://huanwang.tech/">Huan Wang</a></b></br>
            <b>PhD Candidate, Northeastern University</b></br>

            <p><b>Title: </b>Efficient Mobile Text-to-Image Diffusion Models</p>
            <p><b>Abstract: </b>Diffusion models (DMs) trained on enormous text-image pairs, such as Stable Diffusion, DALL-E, have revolutionized the field visual information generation with their exceptional quality. However, the superior quality of these models is offset by their substantial size and the consequent slow inference speed, a challenge that becomes even more pronounced on mobile devices. In this talk, I will first discuss the challenges of running text-to-image DMs on mobile devices. Then, I shall introduce our NeurIPS’23 work, “SnapFusion -- Text-to-Image Diffusion Model on Mobile Devices within Two Seconds”, which is known as the first approach that can achieve text-to-image generation in less than 2 seconds on a mobile device. Particularly, I shall explain how we significantly improve the inference efficiency through a joint optimization of the network architecture and training strategy. Other relevant works (e.g., Google's recent work, MobileDiffusion) will also be discussed. Finally, a summary and outlook of the mobile DMs in the future will conclude the talk.</p>
        </div>



    </div>
    <hr>
    <br><div class="footer-col-wrapper" style="border-radius: 20px; background-color: #F0F0F0;padding-top:16px;padding-bottom:16px;padding-left:32px;padding-right:32px;">
        <div class="footer-col footer-col-1">
            <img src="https://vcla.stat.ucla.edu/images/people/deqiankong.jpg" width="128" height="128">
            <br>
            <p>
                <br>
                <b>Date:</b> 15h March
                <br>
                <b>Time:</b> 13:00 - 14:00 MST
                <br>
                <b>Location:</b> BYENG-361, ASU
                <br>
                <b>YouTube Recooding: </b> <a href="">Link</a>
            </p>
            </p>
            <br>
            <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/University_of_California%2C_Los_Angeles_logo.svg/1280px-University_of_California%2C_Los_Angeles_logo.svg.png" width="128">
        </div>

        <div class="footer-col footer-col-3" style="width:60%">
            <b style="font-size: 21px;"><a href="https://sites.google.com/view/deqiankong/">Deqian Kong</a></b></br>
            <b>PhD Candidate, University of California, Los Angeles</b></br>

            <p><b>Title: </b>Top-down Latent Space Generative Models for Planning and Optimization</p>
            <p><b>Abstract: </b>In this talk, we explore recent developments in latent space generative models, with a focus on our contributions to latent space energy-based models and latent plan transformers. We compare these models to current prevalent methods such as diffusion models and causal transformers, highlighting how our approach aims to offer explicit abstractions for improved generalization, planning, and online optimization. We will discuss two main applications of our work, the adaptation of offline reinforcement learning for planning purposes, and the use of our models in online optimization, particularly with a focus on molecule design and drug discovery. These applications are representative examples of how latent space models can be applied to complex problems, offering potential pathways for further research and exploration in these areas.</p>
        </div>



    </div>
    <hr>
    <br></div> -->

<div>
    <h1 class="page-heading">Organizers</h1>

    

    <div class="footer-col-wrapper"><div class="footer-col" style="padding-right: 60px;">
            <center>
                <img src="assets/images/matt.jpg" width="128" height="128"></br>
                <b><a href="https://maitreyapatel.com/">Maitreya Patel</a></b>
                <!-- <p>PhD Student </br> Arizona State University</p> -->
                <p>Arizona State University</p>
            </center>
        </div><div class="footer-col" style="padding-right: 60px;">
            <center>
                <img src="assets/images/Kyle_profile_crop.jpg" width="128" height="128"></br>
                <b><a href="https://sites.google.com/view/kylemin">Kyle Min</a></b>
                <!-- <p>Staff Research Scientist </br> Intel Labs</p> -->
                <p>Intel Labs</p>
            </center>
        </div><div class="footer-col" style="padding-right: 60px;">
            <center>
                <img src="https://asu-apg.github.io/rbgm/images/changhoonkim.jpg" width="128" height="128"></br>
                <b><a href="https://www.changhoonkim.com/">Changhoon Kim</a></b>
                <!-- <p>Assistant Professor </br> Soongsil University</p> -->
                <p>Soongsil University</p>
            </center>
        </div><div class="footer-col" style="padding-right: 60px;">
            <center>
                <img src="https://pbs.twimg.com/profile_images/1807459775518814208/lveZWIhA_400x400.jpg" width="128" height="128"></br>
                <b><a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a></b>
                <!-- <p>Associate Professor </br> Arizona State University</p> -->
                <p>Arizona State University</p>
            </center>
        </div></div>

</div>
        </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">

        <h2 class="footer-heading">Contact Us</h2>

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list"><ul class="social-media-list"></ul>
<li><a class="u-email" href="mailto:workshop.rbfm@gmail.com, maitreya.patel@asu.edu, ckim79@asu.edu">workshop.rbfm@gmail.com, maitreya.patel@asu.edu, ckim79@asu.edu</a></li></ul>
            </div>

            <div class="footer-col footer-col-2">
            </div>

            <div class="footer-col footer-col-3">
                <p>2nd Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>