---
layout: page
title: Accepted Papers
permalink: /accepted_papers/
---

<h2>Long Papers</h2>
<ol>
  <li>
    <p>[Oral] <b>When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?</b><br/>
    <!-- Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez<br/> -->
    <a href="https://openreview.net/forum?id=jEqdmQYfgg">[Paper]</a></p>
  </li>

  <li>
    <p>[Oral] <b>LLAVAGUARD: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment</b><br/>
    <!-- Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski<br/> -->
    <a href="https://openreview.net/forum?id=Ri2qdOk3Hx">[Paper]</a></p>
  </li>

  <li>
    <p>[Oral] <b>Multimodal Situational Safety</b><br/>
    <!-- Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Anderson Compalas, Xin Eric Wang<br/> -->
    <a href="https://openreview.net/forum?id=L1vaMp6iBC">[Paper]</a></p>
  </li>

  <li>
    <p>[Oral] <b>MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs</b><br/>
    <!-- Wenqian Ye, Guangtao Zheng, Yunsheng Ma, Xu Cao, Bolin Lai, James Matthew Rehg, Aidong Zhang<br/> -->
    <a href="https://openreview.net/forum?id=htCZ3fEdV7">[Paper]</a></p>
  </li>

  <li>
    <p>[Poster] <b>Skipping Computations in Multimodal LLMs</b><br/>
    <!-- Mustafa Shukor, Matthieu Cord<br/> -->
    <a href="https://openreview.net/forum?id=qkmMvLckB9">[Paper]</a></p>
  </li>

  <li>
    <p>[Poster] <b>Coordinated Robustness Evaluation Framework for Vision Language Models</b><br/>
    <!-- Ashwin Ramesh Babu, Sajad Mousavi, Desik Rengarajan, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar<br/> -->
    <a href="https://openreview.net/forum?id=7DpNGceUUV">[Paper]</a></p>
  </li>

  
  <li>
    <p>[Poster] <b>Building and better understanding vision-language models: insights and future directions</b><br />
      <a href="https://openreview.net/forum?id=iSL0FHZStr">[Paper]</a>
    </p>
  </li>
  
  <li>
    <p>[Poster] <b>Incorporating Generative Feedback for Mitigating Hallucinations in Large Vision-Language
        Models</b><br />
      <a href="https://openreview.net/forum?id=1fpjV6xQ6Q">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models</b><br />
      <a href="https://openreview.net/forum?id=2jdohv2bPk">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>LEMoN: Label Error Detection using Multimodal Neighbors</b><br />
      <a href="https://openreview.net/forum?id=Bjm1MzLVRy">[Paper]</a>
    </p>
  </li>
  
  <li>
    <p>[Poster] <b>MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical
        foundation models</b><br />
      <a href="https://openreview.net/forum?id=mKFBNMdNj1">[Paper]</a>
    </p>
  </li>
  
  <li>
    <p>[Poster] <b>Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models</b><br />
      <a href="https://openreview.net/forum?id=hSsgI1gpZ6">[Paper]</a>
    </p>
  </li>
  
  <li>
    <p>[Poster] <b>Decompose, Recompose, and Conquer: Multi-modal LLMs are Vulnerable to Compositional Adversarial
        Attacks in Multi-Image Queries</b><br />
      <a href="https://openreview.net/forum?id=7IXrDfuJTm">[Paper]</a>
    </p>
  </li>
  
  <li>
    <p>[Poster] <b>Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language
        Model</b><br />
      <a href="https://openreview.net/forum?id=wZ4yPytmeK">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and
        Code Tasks</b><br />
      <a href="https://openreview.net/forum?id=UTgNFcpk0j">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs</b><br />
      <a href="https://openreview.net/forum?id=uxUhUiPfQ4">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?</b><br />
      <a href="https://openreview.net/forum?id=VspjUVQsai">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>GUIDE: A Responsible Multimodal Approach for Enhanced Glaucoma Risk Modeling and Patient Trajectory
        Analysis</b><br />
      <a href="https://openreview.net/forum?id=BSdvfyuLaZ">[Paper]</a>
    </p>
  </li>

</ol>

<h2>Short Papers</h2>
<ol>
  <li>
    <p>[Oral] <b>PopAlign: Population-Level Alignment for Fair Text-to-Image Generation</b><br/>
    Shufan Li, Aditya Grover, Harkanwar Singh<br/>
    <a href="https://openreview.net/forum?id=MNxUWFZbkI">[Paper]</a></p>
  </li>

  <li>
    <p>[Oral] <b>Consistency-diversity-realism Pareto fronts of conditional image generative models</b><br/>
    <!-- Pietro Astolfi, Melissa Hall, Jakob Verbeek, Marlene Careil, Oscar MaÃ±as, Matthew J. Muckley, Adriana Romero-Soriano, Michal Drozdzal<br/> -->
    <a href="https://openreview.net/forum?id=H13U2WikCz">[Paper]</a></p>
  </li>

  <li>
    <p><b>Trust but Verify: Reliable VLM evaluation in-the-wild with program synthesis</b><br/>
    <!-- Viraj Uday Prabhu, Senthil Purushwalkam, Jieyu Zhang, An Yan, Caiming Xiong, Ran Xu<br/> -->
    <a href="https://openreview.net/forum?id=P9hTkOJ1wW">[Paper]</a></p>
  </li>

  <li>
    <p>[Poster] <b>You Never Know: Quantization Induces Inconsistent Biases in Vision-Language Foundation
        Models</b><br />
      <a href="https://openreview.net/forum?id=95Bh0yjU9I">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Aligning to What? Limits to RLHF Based Alignment</b><br />
      <a href="https://openreview.net/forum?id=pt2d3SlXQP">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Exploring Intrinsic Fairness in Stable Diffusion</b><br />
      <a href="https://openreview.net/forum?id=nM75a1CzOI">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models</b><br />
      <a href="https://openreview.net/forum?id=4tsFFBvRzX">[Paper]</a>
    </p>
  </li>

  <li>
    <p>[Poster] <b>Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased
        queries</b><br />
      <a href="https://openreview.net/forum?id=4zbj329wz2">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Position Paper: Protocol Learning, Decentralized Frontier Risk and the No-Off Problem</b><br />
      <a href="https://openreview.net/forum?id=xFr6JT2hpy">[Paper]</a>
    </p>
  </li>

  <li>
    <p>[Poster] <b>Comparison Visual Instruction Tuning</b><br />
      <a href="https://openreview.net/forum?id=Mm2mB1jHZK">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Attention Shift: Steering AI Away from Unsafe Content</b><br />
      <a href="https://openreview.net/forum?id=Dbuzp0DjgL">[Paper]</a>
    </p>
  </li>

  <li>
    <p>[Poster] <b>Towards Secure and Private AI: A Framework for Decentralized Inference</b><br />
      <a href="https://openreview.net/forum?id=yAPPj6nGwJ">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models</b><br />
      <a href="https://openreview.net/forum?id=6O8wxERhSF">[Paper]</a>
    </p>
  </li>

  <li>
    <p>[Poster] <b>The Multi-faceted Monosemanticity in Multimodal Representations</b><br />
      <a href="https://openreview.net/forum?id=9NLRpwfLnT">[Paper]</a>
    </p>
  </li>

  <li>
    <p>[Poster] <b>Adversarial Robust Deep Reinforcement Learning is Neither Robust Nor Safe</b><br />
      <a href="https://openreview.net/forum?id=EPa0udvXJE">[Paper]</a>
    </p>
  </li>
  <li>
    <p>[Poster] <b>Probabilistic Active Few-Shot Learning in Vision-Language Models</b><br />
      <a href="https://openreview.net/forum?id=63T4BtNNOM">[Paper]</a>
    </p>
  </li>
</ol>
